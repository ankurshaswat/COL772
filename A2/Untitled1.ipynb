{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dimension):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed1 = nn.Embedding(vocab_size,embedding_dimension)\n",
    "        self.embed1.weight.requires_grad = True\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dimension,vocab_size,bias=False)\n",
    "        self.linear1.weight.requires_grad = True\n",
    "\n",
    "    def set_weights(self,initial_embeds):\n",
    "        self.embed1.weight =  nn.Parameter(initial_embeds)\n",
    "        self.linear1.weight =  nn.Parameter(initial_embeds)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed1(x)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google Model\n",
      "Generating Vocab\n",
      "Reading new domain files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing proper nouns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:32<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new domain tokens\n",
      "Copying old embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Saving Vocab mappings\n",
      "1039106 training examples in one epoch\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import torch\n",
    "import gensim\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from model import Net, window_size\n",
    "from os import listdir\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from tqdm import tqdm as tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "DEBUG = True\n",
    "DEBUG_ITERS = 100\n",
    "\n",
    "pickle_file_name = 'wordIndexes.pkl'\n",
    "model_name = 'model.pt'\n",
    "dataset_path = 'dataset/'\n",
    "embeddings_path = 'GoogleNews-vectors-negative300.bin'\n",
    "embedding_dimension = 300\n",
    "vocab_size = 0\n",
    "epochs = 2\n",
    "batch_size = 1024\n",
    "counts = {}\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "def log(s):\n",
    "    if DEBUG:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "def load():\n",
    "    log('Loading Google Model')\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        embeddings_path, binary=True)\n",
    "\n",
    "    log('Generating Vocab')\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    old_vocab = model.vocab\n",
    "\n",
    "    log('Reading new domain files')\n",
    "\n",
    "    dataset_files = listdir(dataset_path)\n",
    "\n",
    "    data_tokenized = []\n",
    "\n",
    "    for file_path in dataset_files:\n",
    "        with open(dataset_path+'/'+file_path) as file:\n",
    "            data_tokenized.append(word_tokenize(file.read()))\n",
    "\n",
    "    log(\"Replacing proper nouns\")\n",
    "    for i in tqdm(range(len(data_tokenized))):\n",
    "        token_set = data_tokenized[i]\n",
    "        datum_pos_tagged = pos_tag(token_set)\n",
    "        for j in range(len(datum_pos_tagged)):\n",
    "            tag = datum_pos_tagged[j][1]\n",
    "            if(tag == 'NNP' or tag == 'NNPS'):\n",
    "                data_tokenized[i][j] = '-pro-'\n",
    "\n",
    "    log('Adding new domain tokens')\n",
    "    for tokens in data_tokenized:\n",
    "        for token in tokens:\n",
    "            if (token not in word2idx):\n",
    "                word2idx[token] = idx\n",
    "                idx2word[idx] = token\n",
    "                counts[idx] = 1\n",
    "                idx += 1\n",
    "            else:\n",
    "                counts[word2idx[token]] += 1\n",
    "\n",
    "    for token in ['-PADDING-','-UNK-']:\n",
    "        word2idx[token] = idx\n",
    "        idx2word[idx] = token\n",
    "        counts[idx] = 1\n",
    "        idx += 1\n",
    "    \n",
    "    vocab_size = idx\n",
    "\n",
    "    log('Copying old embeddings')\n",
    "    initial_embeds = torch.zeros(vocab_size, embedding_dimension)\n",
    "    for i in range(vocab_size):\n",
    "        if idx2word[i] in old_vocab:\n",
    "            initial_embeds[i, :] = torch.as_tensor(model[idx2word[i]])\n",
    "\n",
    "    log(\"Creating Training Examples\")\n",
    "    train_examples = []\n",
    "    target_words = []\n",
    "    for i in tqdm(range(len(data_tokenized))):\n",
    "        for j in range(len(data_tokenized[i])):\n",
    "            context = []\n",
    "            target_word = word2idx[data_tokenized[i][j]]\n",
    "            for k in range(j-window_size, j+window_size+1):\n",
    "\n",
    "                if(k < 0 or j == k or k >= len(data_tokenized[i])):\n",
    "                    continue\n",
    "\n",
    "                context.append(word2idx[data_tokenized[i][k]])\n",
    "\n",
    "            while(len(context) < 2*window_size):\n",
    "                context.append(word2idx['-PADDING-'])\n",
    "\n",
    "            train_examples.append(context)\n",
    "            target_words.append(target_word)\n",
    "\n",
    "    train_examples = torch.as_tensor(train_examples)\n",
    "    target_words = torch.as_tensor(target_words)\n",
    "\n",
    "    return word2idx, idx2word, vocab_size, embedding_dimension, initial_embeds, train_examples, target_words\n",
    "\n",
    "\n",
    "word2idx, idx2word, vocab_size, embedding_dimension, initial_embeds, train_examples, target_words = load()\n",
    "\n",
    "\n",
    "log('Creating Model')\n",
    "model = Net(vocab_size, embedding_dimension)\n",
    "model.set_weights(initial_embeds)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "log('Saving Vocab mappings')\n",
    "db = {}\n",
    "\n",
    "db['word2idx'] = word2idx\n",
    "db['idx2word'] = idx2word\n",
    "db['embedding_dimension'] = embedding_dimension\n",
    "db['vocab_size'] = vocab_size\n",
    "\n",
    "dbfile = open(pickle_file_name , 'ab')\n",
    "\n",
    "pickle.dump(db, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "log(str(len(train_examples)) + ' training examples in one epoch')\n",
    "\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "dance\n",
      "sing\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "print(idx2word[word2idx['run']])\n",
    "print(idx2word[word2idx['dance']])\n",
    "print(idx2word[word2idx['sing']])\n",
    "print(idx2word[word2idx['love']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0  out of  1039106\r",
      "torch.Size([1024, 300])\n",
      "torch.Size([30309])\n",
      "torch.Size([1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b60f5ef312f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "log('Training')\n",
    "model = Net(vocab_size, embedding_dimension)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    iter_num = 0\n",
    "\n",
    "    for i in range(0, len(target_words), batch_size):\n",
    "\n",
    "        if(DEBUG):\n",
    "            print(i,' out of ',len(target_words), end='\\r')\n",
    "\n",
    "        if(i + batch_size > len(train_examples)):\n",
    "            context_words = train_examples[i:,:]\n",
    "            center_word = target_words[i:]\n",
    "        else:\n",
    "            context_words = train_examples[i:i+batch_size,:]\n",
    "            center_word = target_words[i:i+batch_size]\n",
    "\n",
    "        input_ = context_words\n",
    "        output_ = Variable(center_word)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(input_)\n",
    "        print(outputs.shape)\n",
    "        print(output_.shape)\n",
    "        # Backward\n",
    "        loss = criterion(outputs, output_)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if iter_num % DEBUG_ITERS == DEBUG_ITERS-1:\n",
    "            log(total_loss)\n",
    "            total_loss = 0.0\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "        iter_num = (iter_num + 1) % DEBUG_ITERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
