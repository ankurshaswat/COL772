{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'\n",
    "embeddings_path = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(embeddings_path, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "idx = 0\n",
    "for word in model.vocab:\n",
    "    word2idx[word] = idx\n",
    "    idx2word[idx] = word\n",
    "    idx += 1\n",
    "\n",
    "trained_words = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = listdir(dataset_path)\n",
    "\n",
    "data_tokenized = []\n",
    "\n",
    "for file_path in dataset_files:\n",
    "    with open(dataset_path+'/'+file_path) as file:\n",
    "        data_tokenized.append(word_tokenize(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing proper nouns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcbb670196d41cdba8530e0627c9e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Replacing proper nouns\")\n",
    "for i in tqdm(range(len(data_tokenized))):\n",
    "        token_set = data_tokenized[i]\n",
    "        datum_pos_tagged = pos_tag(token_set)\n",
    "        for j in range(len(datum_pos_tagged)):\n",
    "            tag = datum_pos_tagged[j][1]\n",
    "            if(tag == 'NNP' or tag == 'NNPS'):\n",
    "                data_tokenized[i][j] = '-pro-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3006482\n"
     ]
    }
   ],
   "source": [
    "for tokens in data_tokenized:\n",
    "    for token in tokens:\n",
    "        if (token not in word2idx):\n",
    "            print(token)\n",
    "            word2idx[token] = idx\n",
    "            idx2word[idx] = token\n",
    "            idx += 1\n",
    "\n",
    "vocab_size = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = model[idx2word[0]].shape[0]\n",
    "\n",
    "initial_embeds = torch.randn(vocab_size,embedding_dimension)\n",
    "initial_embeds[:trained_words,:] = torch.as_tensor(model[model.vocab])\n",
    "# context, target\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing proper nouns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3e5df18ef4088b442eb0a5dcf4f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Creating Training Examples\")\n",
    "\n",
    "train_examples = []\n",
    "target_words = []\n",
    "for i in tqdm(range(len(data_tokenized))):\n",
    "    for j in range(len(data_tokenized[i])):\n",
    "        for k in range(j-window_size,j+window_size+1):\n",
    "            if(k<0 or j==k or k>=len(data_tokenized[i])):\n",
    "                continue\n",
    "            train_examples.append(word2idx[data_tokenized[i][k]])\n",
    "            target_words.append(word2idx[data_tokenized[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed1 = nn.Embedding(vocab_size,embedding_dimension)\n",
    "        self.embed1.weight.requires_grad = True\n",
    "        self.linear1 = nn.Linear(embedding_dimension,vocab_size,bias=False)\n",
    "        self.linear1.weight.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed1(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "debug_iters = 2000\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(train_examples)):\n",
    "        context_words = train_examples[i]\n",
    "        center_word = target_words[i]\n",
    "        \n",
    "        input_ = torch.tensor([context_words])\n",
    "        output_ = Variable(torch.from_numpy(np.array([center_word])).long())\n",
    "\n",
    "        \n",
    "        # Forward\n",
    "        outputs = net(torch.tensor([input_]))\n",
    "        log_softmax = F.log_softmax(outputs)\n",
    "        \n",
    "        # Backward\n",
    "        loss = F.nll_loss(log_softmax,output_)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % debug_iters == debug_iters-1:\n",
    "            print(total_loss)\n",
    "            total_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
