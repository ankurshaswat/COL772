{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'\n",
    "embedding_dimension = 100\n",
    "embeddings_path = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7231419673134733b310285093d52ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76f38538c544e0d99de6d3997c466c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_files = listdir(dataset_path)\n",
    "\n",
    "data_tokenized = []\n",
    "\n",
    "for file_path in dataset_files:\n",
    "    with open(dataset_path+'/'+file_path) as file:\n",
    "        data_tokenized.append(word_tokenize(file.read()))\n",
    "\n",
    "for i in tqdm(range(len(data_tokenized))):\n",
    "        token_set = data_tokenized[i]\n",
    "        datum_pos_tagged = pos_tag(token_set)\n",
    "        for j in range(len(datum_pos_tagged)):\n",
    "            tag = datum_pos_tagged[j][1]\n",
    "            if(tag == 'NNP' or tag == 'NNPS'):\n",
    "                data_tokenized[i][j] = '-pro-'\n",
    "\n",
    "vocab = set([])\n",
    "for data in data_tokenized:\n",
    "    for token in data:\n",
    "        vocab.add(token)\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "idx = 0\n",
    "for word in vocab:\n",
    "    word2idx[word] = idx\n",
    "    idx2word[idx] = word\n",
    "    idx += 1\n",
    "    \n",
    "idx -= 1\n",
    "vocab_size = idx\n",
    "initial_embeds = torch.rand(vocab_size,embedding_dimension)\n",
    "\n",
    "# context, target\n",
    "train_examples = []\n",
    "window_size = 2\n",
    "\n",
    "for i in tqdm(range(len(data_tokenized))):\n",
    "    for j in range(len(data_tokenized[i])):\n",
    "        for k in range(j-window_size,j+window_size+1):\n",
    "            if(k<0 or j==k or k>=len(data_tokenized[i])):\n",
    "                continue\n",
    "            train_examples.append((word2idx[data_tokenized[i][k]],word2idx[data_tokenized[i][j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(embeddings_path, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Vocab(count:2999791, index:209)\n"
     ]
    }
   ],
   "source": [
    "# print(model.wv.vocab['run'])\n",
    "print(model['run'].shape[0])\n",
    "print(model.vocab['run'])\n",
    "# word2idx[word] = idx\n",
    "# idx2word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed1.weight tensor([[-1.2616, -0.2915,  2.1652,  ...,  1.6965,  0.4838, -1.5713],\n",
      "        [ 0.6393, -0.1153,  2.4186,  ...,  0.4038,  0.0099, -1.0737],\n",
      "        [-0.7282, -0.6344, -0.1435,  ..., -1.7812, -0.4675, -0.1092],\n",
      "        ...,\n",
      "        [-0.2661, -0.1388,  1.4205,  ...,  0.1541,  0.3668, -0.1108],\n",
      "        [-1.3831, -0.7770,  0.0425,  ..., -1.6575, -0.1327, -0.4992],\n",
      "        [ 0.7098,  0.4363, -0.6173,  ..., -1.0384, -0.2999,  0.0213]])\n",
      "linear1.weight tensor([[ 0.0302, -0.0172,  0.0684,  ..., -0.0449,  0.0554, -0.0743],\n",
      "        [-0.0765,  0.0462, -0.0768,  ..., -0.0096, -0.0213,  0.0757],\n",
      "        [-0.0556, -0.0989, -0.0505,  ...,  0.0586, -0.0982,  0.0789],\n",
      "        ...,\n",
      "        [-0.0814, -0.0721,  0.0699,  ...,  0.0469, -0.0058, -0.0259],\n",
      "        [-0.0484,  0.0144,  0.0023,  ..., -0.0634,  0.0630, -0.0778],\n",
      "        [-0.0473, -0.0545, -0.0820,  ..., -0.0897, -0.0772, -0.0822]])\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed1 = nn.Embedding(vocab_size,embedding_dimension)\n",
    "        self.embed1.weight.requires_grad = True\n",
    "        self.linear1 = nn.Linear(embedding_dimension,vocab_size,bias=False)\n",
    "        self.linear1.weight.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed1(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "    print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankurshaswat/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12191.20909166336\n",
      "15487.184662342072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-6557d7720913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyFiles/Repos/COL772/A1_part_1/env/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "debug_iters = 2000\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(train_examples)):\n",
    "        context_words, center_word = train_examples[i]\n",
    "        \n",
    "        input_ = torch.tensor([context_words])\n",
    "        output_ = Variable(torch.from_numpy(np.array([center_word])).long())\n",
    "\n",
    "        \n",
    "        # Forward\n",
    "        outputs = net(torch.tensor([input_]))\n",
    "        log_softmax = F.log_softmax(outputs)\n",
    "        \n",
    "        # Backward\n",
    "        loss = F.nll_loss(log_softmax,output_)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % debug_iters == debug_iters-1:\n",
    "            print(total_loss)\n",
    "            total_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed1.weight tensor([[-1.2616, -0.2915,  2.1652,  ...,  1.6965,  0.4838, -1.5713],\n",
      "        [ 0.6393, -0.1153,  2.4186,  ...,  0.4038,  0.0099, -1.0737],\n",
      "        [-0.7282, -0.6344, -0.1435,  ..., -1.7812, -0.4675, -0.1092],\n",
      "        ...,\n",
      "        [-0.2661, -0.1388,  1.4205,  ...,  0.1541,  0.3668, -0.1108],\n",
      "        [-1.3831, -0.7770,  0.0425,  ..., -1.6575, -0.1327, -0.4992],\n",
      "        [ 0.7098,  0.4363, -0.6173,  ..., -1.0384, -0.2999,  0.0213]])\n",
      "linear1.weight tensor([[ 0.0302, -0.0173,  0.0682,  ..., -0.0449,  0.0550, -0.0740],\n",
      "        [-0.0764,  0.0460, -0.0768,  ..., -0.0097, -0.0216,  0.0753],\n",
      "        [-0.0557, -0.0987, -0.0504,  ...,  0.0584, -0.0979,  0.0785],\n",
      "        ...,\n",
      "        [-0.0820, -0.0722,  0.0692,  ...,  0.0465, -0.0058, -0.0256],\n",
      "        [-0.0483,  0.0142,  0.0022,  ..., -0.0634,  0.0628, -0.0774],\n",
      "        [-0.0474, -0.0542, -0.0818,  ..., -0.0898, -0.0771, -0.0814]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "    print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156346\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
